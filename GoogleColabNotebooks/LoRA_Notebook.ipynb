{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "J0vFebCOfHKk",
        "outputId": "41802240-bd19-4a3a-ea12-3c4268a4069f"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 1) Synthetic task -------------------------------------------------\n",
        "k, d, r = 128, 128, 32 # k is output , d is input , and r is rank\n",
        "\n",
        "W_base  = torch.randn(k, d)                 # \"pre-trained\" frozen weight\n",
        "A_true  = torch.randn(r, d)\n",
        "B_true  = torch.randn(k, r)\n",
        "W_true  = W_base + B_true @ A_true          # ground-truth weight\n",
        "\n",
        "#print(W_base.shape) # 128 x 128\n",
        "#print(A_true.shape) # 32 x 128\n",
        "#print(B_true.shape) # 128 x 32\n",
        "#print(W_true.shape) # 128 x 128\n",
        "\n",
        "N = 10_000\n",
        "X = torch.randn(N, d)\n",
        "y = X @ W_true.t() # 10,000 x 128\n",
        "print(W_true.t().shape) # 128 x 128\n",
        "#print(y.shape) # 10,000 x 128  output of the model\n",
        "\n",
        "\n",
        "# 2) Models ---------------------------------------------------------\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, W_frozen, rank):\n",
        "        super().__init__()\n",
        "        k, d = W_frozen.shape # 128 x 128 it is from W_base, k is output d is input\n",
        "        self.register_buffer(\"W\", W_frozen)\n",
        "        self.A = nn.Parameter(0.01 * torch.randn(rank, d)) # 32 x 128\n",
        "        self.B = nn.Parameter(torch.zeros(k, rank)) # 128 x 32\n",
        "        print(f\"W_frozen shape: {W_frozen.shape}\")  # [k, d]\n",
        "        print(f\"A shape       : {self.A.shape}\")     # [r, d]\n",
        "        print(f\"B shape       : {self.B.shape}\")     # [k, r]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ (self.W + self.B @ self.A).t()\n",
        "\n",
        "class FullLinear(nn.Module):\n",
        "    def __init__(self, W_init):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(W_init.clone())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.W.t()\n",
        "\n",
        "model_lora = LoRALinear(W_base, r)\n",
        "model_full = FullLinear(W_base)\n",
        "\n",
        "print(f\"LoRA trainable params: {sum(p.numel() for p in model_lora.parameters() if p.requires_grad):,}\")\n",
        "print(f\"Full-FT params      : {sum(p.numel() for p in model_full.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# 3) Training -------------------------------------------------------\n",
        "def train(model, steps=5000, batch=256, lr=1e-2):\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    mse = nn.MSELoss()\n",
        "    losses = []\n",
        "    for s in range(steps):\n",
        "        idx = torch.randint(0, N, (batch,)) #256\n",
        "        loss = mse(model(X[idx]), y[idx])\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        losses.append(loss.item())\n",
        "    return losses\n",
        "\n",
        "loss_lora = train(model_lora)\n",
        "loss_full = train(model_full)\n",
        "\n",
        "# 4) Plots ----------------------------------------------------------\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
        "\n",
        "ax[0].plot(loss_full, label=\"Full Fine-Tune\")\n",
        "ax[0].plot(loss_lora, label=f\"LoRA (r={r})\")\n",
        "ax[0].set_yscale(\"log\"); ax[0].set_xlabel(\"Step\"); ax[0].set_ylabel(\"Batch MSE\")\n",
        "ax[0].set_title(\"Training Loss\"); ax[0].legend()\n",
        "\n",
        "ax[1].bar([\"Full\\n(16 384)\", \"LoRA\\n(2 048)\"],\n",
        "          [16_384, r*(k+d)])\n",
        "ax[1].set_ylabel(\"# trainable weights\")\n",
        "ax[1].set_title(\"Parameter Budget\")\n",
        "\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# 5) How close to the true ΔW? -------------------------------------\n",
        "with torch.no_grad():\n",
        "    ΔW_true = B_true @ A_true\n",
        "    ΔW_lora = model_lora.B @ model_lora.A\n",
        "    ΔW_full = model_full.W - W_base\n",
        "    mse = lambda a,b: ((a-b)**2).mean().item()\n",
        "    print(f\"ΔW MSE  LoRA : {mse(ΔW_lora, ΔW_true):.2e}\")\n",
        "    print(f\"ΔW MSE  Full : {mse(ΔW_full , ΔW_true):.2e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
