{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvUSKgAS7sFC",
        "outputId": "f6020adc-9448-4e8e-9761-80a1bf8db82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of float32 tensor: 3.73 GB\n",
            "Size of bfloat16 tensor: 1.86 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a large random tensor with 1 billion elements\n",
        "# This simulates the size of a large model's parameters\n",
        "num_elements = 1_000_000_000\n",
        "\n",
        "# Tensor in standard float32 format\n",
        "tensor_fp32 = torch.randn(num_elements, dtype=torch.float32)\n",
        "\n",
        "# Tensor in bfloat16 format\n",
        "# We create it in fp32 first and then convert it, which is a common practice\n",
        "tensor_bf16 = tensor_fp32.to(dtype=torch.bfloat16)\n",
        "\n",
        "# Calculate the memory size in Gigabytes (GB)\n",
        "size_fp32_gb = tensor_fp32.element_size() * tensor_fp32.nelement() / (1024**3)\n",
        "size_bf16_gb = tensor_bf16.element_size() * tensor_bf16.nelement() / (1024**3)\n",
        "\n",
        "print(f\"Size of float32 tensor: {size_fp32_gb:.2f} GB\")\n",
        "print(f\"Size of bfloat16 tensor: {size_bf16_gb:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# A number with lots of decimal detail\n",
        "original_number = 3.141592653523452345235235234523452352345234523452352345234\n",
        "\n",
        "# Store it in both formats\n",
        "num_fp32 = torch.tensor(original_number, dtype=torch.float32)\n",
        "num_bf16 = torch.tensor(original_number, dtype=torch.bfloat16)\n",
        "\n",
        "print(f\"Original Number:      {original_number}\")\n",
        "print(f\"Stored as float32:    {num_fp32.item()}\")\n",
        "print(f\"Stored as bfloat16:   {num_bf16.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AeA_tEG7zU-",
        "outputId": "9396e466-b033-43e0-93a7-bafc1d639c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Number:      3.1415926535234524\n",
            "Stored as float32:    3.1415927410125732\n",
            "Stored as bfloat16:   3.140625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Ensure we are using a GPU\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Please enable GPU in Runtime -> Change runtime type\")\n",
        "else:\n",
        "    # Create large matrices on the GPU\n",
        "    matrix_size = 16000\n",
        "    a = torch.randn(matrix_size, matrix_size, device='cuda')\n",
        "    b = torch.randn(matrix_size, matrix_size, device='cuda')\n",
        "\n",
        "    # Time the float32 multiplication\n",
        "    start_time_fp32 = time.time()\n",
        "    torch.matmul(a, b)\n",
        "    end_time_fp32 = time.time()\n",
        "    print(f\"Time for float32 multiplication: {end_time_fp32 - start_time_fp32:.4f} seconds\")\n",
        "\n",
        "    # Convert matrices to bfloat16\n",
        "    a_bf16 = a.to(torch.bfloat16)\n",
        "    b_bf16 = b.to(torch.bfloat16)\n",
        "\n",
        "    # Time the bfloat16 multiplication\n",
        "    # We use torch.cuda.amp.autocast to ensure hardware acceleration is used\n",
        "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "        start_time_bf16 = time.time()\n",
        "        torch.matmul(a_bf16, b_bf16)\n",
        "        end_time_bf16 = time.time()\n",
        "        print(f\"Time for bfloat16 multiplication: {end_time_bf16 - start_time_bf16:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My17JdK879Te",
        "outputId": "29f52bc5-81f1-4560-d07e-30c2cef1dfcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for float32 multiplication: 0.0002 seconds\n",
            "Time for bfloat16 multiplication: 0.0001 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-13-521710501.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        }
      ]
    }
  ]
}